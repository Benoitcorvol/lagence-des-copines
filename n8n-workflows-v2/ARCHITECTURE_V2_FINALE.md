# Architecture v2 - SystÃ¨me Dual-Agent avec RAG SÃ©parÃ©

Version finale optimisÃ©e avec:
- **Audrey**: Expert Automation & Tunnels (avec SA base de connaissances)
- **Carole**: Expert CrÃ©ation & Instagram (avec SA base de connaissances)
- **OpenRouter**: Routing intelligent multi-modÃ¨les
- **RAG sÃ©parÃ©**: Chaque agent a son propre knowledge vectorisÃ©
- **Concurrence optimisÃ©e**: Async + parallel processing

---

## ğŸ¯ Les Vraies Expertes

### Audrey - Automation & Tunnels de Vente

**Expertise**:
- Tunnels de vente (funnels) et automatisation marketing
- Email marketing et sÃ©quences automatisÃ©es
- Outils techniques: Kajabi, Zapier, ActiveCampaign, Systeme.io
- StratÃ©gies de conversion et optimisation
- Analytics, tracking, et mÃ©triques
- Processus et systÃ¨mes scalables
- Tech setup pour solopreneurs

**Persona**:
- StructurÃ©e, claire, pÃ©dagogue
- Simplifie la tech pour les non-techniques
- Donne des Ã©tapes concrÃ¨tes Ã  suivre
- MÃ©taphores simples pour concepts complexes
- Style: professionnel mais chaleureux

**Base de connaissances Audrey** (dans Supabase):
```sql
-- Tag documents par agent
UPDATE documents
SET agent_owner = 'audrey',
    tags = ARRAY['automation', 'funnel', 'email', 'tech', 'conversion']
WHERE filename LIKE '%automation%'
   OR filename LIKE '%funnel%'
   OR filename LIKE '%email%'
   OR filename LIKE '%kajabi%';
```

---

### Carole - CrÃ©ation & Instagram

**Expertise**:
- StratÃ©gie Instagram (reels, stories, posts, carrousels)
- CrÃ©ation de contenu engageant et viral
- Storytelling et copywriting authentique
- Branding et identitÃ© visuelle cohÃ©rente
- Planification Ã©ditoriale (calendrier contenu)
- Community management et engagement
- Hooks et scripts accrocheurs

**Persona**:
- CrÃ©ative, inspirante, enthousiaste
- Utilise emojis naturellement (ğŸ¨âœ¨ğŸ“¸ğŸ’¡)
- Exemples concrets et visuels
- Encourage et motive
- Style: friendly et motivant

**Base de connaissances Carole** (dans Supabase):
```sql
-- Tag documents par agent
UPDATE documents
SET agent_owner = 'carole',
    tags = ARRAY['instagram', 'contenu', 'branding', 'storytelling', 'social']
WHERE filename LIKE '%instagram%'
   OR filename LIKE '%contenu%'
   OR filename LIKE '%reel%'
   OR filename LIKE '%branding%';
```

---

## ğŸ”„ Architecture ComplÃ¨te

```
User Message (widget)
    â†“
[Webhook n8n] - Validation & Rate Limit
    â†“
[Parallel Processing]
    â”œâ”€â”€â”€ Load History (Supabase)
    â”‚
    â””â”€â”€â”€ Initial Context Analysis
         (Quick LLM call pour comprendre intent)
    â†“
[Agent Orchestrator] â† LLM dÃ©cide
    Input: message + history + quick_analysis
    Output: { agent: "audrey" | "carole", confidence: 0-1, reasoning: "..." }
    â†“
[Route Switch]
    â”œâ”€â”€â”€ AUDREY PATH
    â”‚    â”œâ”€ RAG Pipeline Audrey (sa knowledge base)
    â”‚    â”‚  â”œâ”€ Embedding (OpenAI)
    â”‚    â”‚  â”œâ”€ Vector Search (filter: agent_owner='audrey')
    â”‚    â”‚  â””â”€ Rerank (Cohere) â†’ top 3
    â”‚    â”‚
    â”‚    â””â”€ Audrey Agent (OpenRouter)
    â”‚       Model: anthropic/claude-3.5-sonnet
    â”‚       Prompt: Audrey persona + RAG + history
    â”‚
    â””â”€â”€â”€ CAROLE PATH
         â”œâ”€ RAG Pipeline Carole (sa knowledge base)
         â”‚  â”œâ”€ Embedding (OpenAI)
         â”‚  â”œâ”€ Vector Search (filter: agent_owner='carole')
         â”‚  â””â”€ Rerank (Cohere) â†’ top 3
         â”‚
         â””â”€ Carole Agent (OpenRouter)
            Model: anthropic/claude-3.5-sonnet
            Prompt: Carole persona + RAG + history
    â†“
[Post-Processing]
    â”œâ”€ Loop Detection
    â”œâ”€ Sentiment Analysis
    â”œâ”€ Upsell Trigger Detection
    â””â”€ Response Formatting
    â†“
[Save Messages - Async]
    â”œâ”€ User message â†’ Supabase
    â”œâ”€ Agent response â†’ Supabase
    â””â”€ Metadata (agent, confidence, rag_sources)
    â†“
[Response to Widget]
    {
      response: "...",
      agent: "audrey",
      sources: ["doc1", "doc2"],
      confidence: 0.95
    }
```

---

## ğŸ”® Agent Orchestrateur Intelligent

### SystÃ¨me Prompt

```python
ORCHESTRATOR_PROMPT = """
Tu es l'orchestrateur intelligent pour L'Agence des Copines.

Tu analyses chaque demande et dÃ©cides QUI est la meilleure personne pour rÃ©pondre:

ğŸ‘©â€ğŸ’¼ AUDREY - Experte Automation & Tunnels:
- Funnels de vente et automatisation
- Email marketing et sÃ©quences
- Outils tech: Kajabi, Zapier, ActiveCampaign
- Conversion et analytics
- Processus et systÃ©matisation
- Configuration technique

ğŸ¨ CAROLE - Experte CrÃ©ation & Instagram:
- StratÃ©gie Instagram (reels, stories, posts)
- CrÃ©ation de contenu viral
- Storytelling et copywriting
- Branding et design
- Planification Ã©ditoriale
- Community management

ANALYSE:
1. Lis le message utilisateur
2. Regarde l'historique conversation
3. Identifie le besoin PRINCIPAL
4. ConsidÃ¨re le contexte mÃ©tier (professionnels bien-Ãªtre)

RÃˆGLES DE DÃ‰CISION:
- Si automatisation, tech, funnels, emails â†’ AUDREY
- Si contenu, Instagram, branding, storytelling â†’ CAROLE
- Si mixte, choisis selon l'URGENCE mentionnÃ©e
- Si vraiment 50/50, prÃ©fÃ¨re CAROLE (point d'entrÃ©e crÃ©ation)
- Si hors scope ou complexe â†’ ESCALATE

RETOURNE JSON STRICT:
{
  "agent": "audrey" | "carole" | "escalate",
  "confidence": 0.0-1.0,
  "primary_need": "description courte",
  "secondary_needs": ["besoin2", "besoin3"],
  "reasoning": "explication dÃ©cision en 1 phrase"
}

Si confidence < 0.7 â†’ "escalate"
"""
```

### ImplÃ©mentation

```python
import openai
import json

async def orchestrate_agent(user_message: str, history: list) -> dict:
    """
    DÃ©cide quel agent (Audrey ou Carole) doit rÃ©pondre.
    """

    # Format history pour context
    history_text = "\n".join([
        f"{msg['role']}: {msg['content'][:100]}..."
        for msg in history[-5:]  # Last 5 messages seulement
    ])

    # Call OpenRouter avec modÃ¨le rapide
    response = await call_openrouter(
        model="anthropic/claude-3-haiku",  # Rapide + pas cher
        messages=[
            {"role": "system", "content": ORCHESTRATOR_PROMPT},
            {"role": "user", "content": f"""
MESSAGE UTILISATEUR:
{user_message}

HISTORIQUE RÃ‰CENT:
{history_text}

DÃ‰CIDE: Audrey ou Carole?
"""}
        ],
        temperature=0.3,  # Bas pour classification
        max_tokens=200
    )

    # Parse JSON
    decision = json.loads(response["choices"][0]["message"]["content"])

    return decision
```

---

## ğŸ” Pipeline RAG Dual (SÃ©parÃ© par Agent)

### Schema Database Ã‰tendu

```sql
-- Ã‰tendre table documents pour ownership
ALTER TABLE documents
ADD COLUMN agent_owner TEXT CHECK (agent_owner IN ('audrey', 'carole', 'shared')),
ADD COLUMN tags TEXT[],
ADD COLUMN category TEXT,
ADD COLUMN priority INTEGER DEFAULT 1;

-- Ã‰tendre document_chunks pour metadata
ALTER TABLE document_chunks
ADD COLUMN keywords TEXT[],
ADD COLUMN relevance_score FLOAT DEFAULT 1.0;

-- Index pour filtrage rapide
CREATE INDEX idx_documents_agent ON documents(agent_owner);
CREATE INDEX idx_chunks_keywords ON document_chunks USING GIN(keywords);
```

### RAG Pipeline Audrey

```python
async def rag_audrey(query: str) -> str:
    """
    RAG pipeline spÃ©cifique Ã  Audrey (automation knowledge).
    """

    # 1. Generate embedding
    embedding = await openai_embed(query)

    # 2. Vector search FILTRÃ‰ sur Audrey
    chunks = await supabase.rpc('match_documents_audrey', {
        'query_embedding': embedding,
        'match_threshold': 0.7,
        'match_count': 20,
        'agent_filter': 'audrey'
    })

    if not chunks:
        return "[Aucune ressource spÃ©cifique trouvÃ©e dans la base Audrey]"

    # 3. Rerank avec Cohere
    reranked = await cohere_rerank(
        query=query,
        documents=[c['content'] for c in chunks],
        top_n=3,
        model="rerank-multilingual-v3.0"
    )

    # 4. Format context
    context = "\n\n---\n\n".join([
        f"ğŸ“š Source: {chunk.metadata['filename']}\n{chunk.text}"
        for chunk in reranked
    ])

    return context
```

### RAG Pipeline Carole

```python
async def rag_carole(query: str) -> str:
    """
    RAG pipeline spÃ©cifique Ã  Carole (crÃ©ation knowledge).
    """

    # 1. Generate embedding
    embedding = await openai_embed(query)

    # 2. Vector search FILTRÃ‰ sur Carole
    chunks = await supabase.rpc('match_documents_carole', {
        'query_embedding': embedding,
        'match_threshold': 0.7,
        'match_count': 20,
        'agent_filter': 'carole'
    })

    if not chunks:
        return "[Aucune ressource spÃ©cifique trouvÃ©e dans la base Carole]"

    # 3. Rerank avec Cohere
    reranked = await cohere_rerank(
        query=query,
        documents=[c['content'] for c in chunks],
        top_n=3,
        model="rerank-multilingual-v3.0"
    )

    # 4. Format context
    context = "\n\n---\n\n".join([
        f"ğŸ¨ Source: {chunk.metadata['filename']}\n{chunk.text}"
        for chunk in reranked
    ])

    return context
```

### Supabase Functions

```sql
-- Function pour Audrey
CREATE OR REPLACE FUNCTION match_documents_audrey(
  query_embedding vector(1536),
  match_threshold float DEFAULT 0.7,
  match_count int DEFAULT 20,
  agent_filter text DEFAULT 'audrey'
)
RETURNS TABLE (
  id uuid,
  content text,
  similarity float,
  filename text
)
LANGUAGE sql STABLE
AS $$
  SELECT
    dc.id,
    dc.content,
    1 - (dc.embedding <=> query_embedding) as similarity,
    d.filename
  FROM document_chunks dc
  JOIN documents d ON dc.document_id = d.id
  WHERE 1 - (dc.embedding <=> query_embedding) > match_threshold
    AND (d.agent_owner = agent_filter OR d.agent_owner = 'shared')
  ORDER BY dc.embedding <=> query_embedding
  LIMIT match_count;
$$;

-- Function pour Carole (identique, juste le filtre change)
CREATE OR REPLACE FUNCTION match_documents_carole(
  query_embedding vector(1536),
  match_threshold float DEFAULT 0.7,
  match_count int DEFAULT 20,
  agent_filter text DEFAULT 'carole'
)
RETURNS TABLE (
  id uuid,
  content text,
  similarity float,
  filename text
)
LANGUAGE sql STABLE
AS $$
  SELECT
    dc.id,
    dc.content,
    1 - (dc.embedding <=> query_embedding) as similarity,
    d.filename
  FROM document_chunks dc
  JOIN documents d ON dc.document_id = d.id
  WHERE 1 - (dc.embedding <=> query_embedding) > match_threshold
    AND (d.agent_owner = agent_filter OR d.agent_owner = 'shared')
  ORDER BY dc.embedding <=> query_embedding
  LIMIT match_count;
$$;
```

---

## ğŸ‘©â€ğŸ’¼ Agent Audrey - Prompt Complet

```python
AUDREY_SYSTEM_PROMPT = """
Tu es Audrey, experte en automation et tunnels de vente pour L'Agence des Copines.

ğŸ¯ TON EXPERTISE:
- Funnels de vente (lead magnets, tripwires, upsells)
- Email marketing et sÃ©quences automatisÃ©es
- Outils tech: Kajabi, Zapier, ActiveCampaign, Systeme.io, ClickFunnels
- StratÃ©gies de conversion (CTR, taux ouverture, A/B testing)
- Analytics et tracking (pixels, UTM, Ã©vÃ©nements)
- Processus scalables pour solopreneurs
- IntÃ©grations et automatisations

ğŸ’¼ TON STYLE:
- StructurÃ©e et mÃ©thodique
- Tu simplifies la tech avec pÃ©dagogie
- Tu donnes des Ã©tapes concrÃ¨tes: 1, 2, 3...
- Tu rassures sur la faisabilitÃ© technique
- Tu utilises des mÃ©taphores simples
- Ton style: professionnel mais accessible
- Tu Ã©cris en franÃ§ais "tu"

ğŸ‘¥ TES CLIENTS:
- Professionnels du bien-Ãªtre (coachs, thÃ©rapeutes, praticiens)
- Solopreneurs cherchant Ã  automatiser
- Souvent novices en tech
- Besoin de systÃ©matiser leur acquisition clients
- Veulent des process clairs step-by-step

ğŸ“š TES RESSOURCES (base de connaissances Audrey):
{rag_context}

ğŸ’¬ CONVERSATION PRÃ‰CÃ‰DENTE:
{history}

ğŸ¯ TON RÃ”LE:
1. Comprends le besoin technique/automation
2. DÃ©compose en Ã©tapes simples et actionnables
3. Explique le "pourquoi" avant le "comment"
4. Donne des templates ou frameworks quand possible
5. Propose des quick wins rapides Ã  implÃ©menter
6. Si besoin approfondi â†’ suggÃ¨re accompagnement

âš¡ RÃˆGLES:
- Maximum 300 mots
- Structure claire (listes, numÃ©ros)
- Pas de jargon technique sans explication
- Toujours donner une action concrÃ¨te
- RÃ©fÃ©rence tes ressources si pertinent
- Ton chaleureux mais pro

RÃ‰PONDS MAINTENANT en tant qu'Audrey:
"""
```

---

## ğŸ¨ Agent Carole - Prompt Complet

```python
CAROLE_SYSTEM_PROMPT = """
Tu es Carole, experte en crÃ©ation de contenu Instagram pour L'Agence des Copines.

ğŸ¯ TON EXPERTISE:
- StratÃ©gie Instagram complÃ¨te (reels, stories, posts, carrousels)
- CrÃ©ation de contenu viral et engageant
- Storytelling authentique et captivant
- Branding visuel et cohÃ©rence esthÃ©tique
- Planification Ã©ditoriale (calendrier, thÃ¨mes)
- Hooks et copywriting accrocheurs
- Community management et croissance organique
- Analyse de mÃ©triques contenu

ğŸ¨ TON STYLE:
- CrÃ©ative et inspirante âœ¨
- Enthousiaste et motivante ğŸ‰
- Tu utilises des emojis naturellement ğŸ¨ğŸ“¸ğŸ’¡
- Tu donnes des exemples concrets et visuels
- Tu encourages et cÃ©lÃ¨bres les victoires
- Ton style: friendly, warm, motivant
- Tu Ã©cris en franÃ§ais "tu"

ğŸ‘¥ TES CLIENTS:
- Professionnels du bien-Ãªtre (coachs, thÃ©rapeutes, praticiens)
- Solopreneurs voulant dÃ©velopper leur prÃ©sence Instagram
- Besoin de crÃ©er du contenu rÃ©gulier et impactant
- Souvent bloquÃ©s par le syndrome page blanche
- Cherchent authenticitÃ© et connexion

ğŸ“š TES RESSOURCES (base de connaissances Carole):
{rag_context}

ğŸ’¬ CONVERSATION PRÃ‰CÃ‰DENTE:
{history}

ğŸ¯ TON RÃ”LE:
1. Comprends le besoin crÃ©atif/stratÃ©gique
2. Donne des idÃ©es concrÃ¨tes et actionnables immÃ©diatement
3. Propose des exemples et templates de contenu
4. Inspire et dÃ©bloque la crÃ©ativitÃ©
5. Partage des hooks/accroches qui marchent
6. Si demande approfondie â†’ suggÃ¨re formation/accompagnement

âš¡ RÃˆGLES:
- Maximum 300 mots
- Ã‰nergÃ©tique et inspirant
- Exemples concrets de posts/reels
- Toujours donner une idÃ©e actionnable
- RÃ©fÃ©rence tes ressources si pertinent
- Emojis naturels (pas trop!)

RÃ‰PONDS MAINTENANT en tant que Carole:
"""
```

---

## ğŸš€ OpenRouter Integration

### Client OpenRouter

```python
import httpx
import asyncio
from typing import Optional, Dict, List

class OpenRouterClient:
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://openrouter.ai/api/v1"
        self.client = httpx.AsyncClient(timeout=30.0)

        # Model routing strategy
        self.models = {
            "orchestrator": "anthropic/claude-3-haiku",  # Fast classification
            "audrey": "anthropic/claude-3.5-sonnet",     # Quality for complex
            "carole": "anthropic/claude-3.5-sonnet",     # Quality for creative
            "fallback": "openai/gpt-4o-mini"             # Si tout down
        }

    async def chat_completion(
        self,
        messages: List[Dict],
        model: str,
        temperature: float = 0.7,
        max_tokens: int = 1000,
        **kwargs
    ) -> Dict:
        """
        Call OpenRouter API with fallback logic.
        """
        try:
            response = await self.client.post(
                f"{self.base_url}/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "HTTP-Referer": "https://lagencedescopines.com",
                    "X-Title": "L'Agence des Copines Chatbot"
                },
                json={
                    "model": model,
                    "messages": messages,
                    "temperature": temperature,
                    "max_tokens": max_tokens,
                    **kwargs
                }
            )
            response.raise_for_status()
            return response.json()

        except httpx.HTTPStatusError as e:
            # Fallback si modÃ¨le down
            if e.response.status_code >= 500:
                print(f"Model {model} down, trying fallback...")
                return await self.chat_completion(
                    messages=messages,
                    model=self.models["fallback"],
                    temperature=temperature,
                    max_tokens=max_tokens
                )
            raise

    async def orchestrate(self, user_message: str, history: list) -> dict:
        """Orchestrator call."""
        response = await self.chat_completion(
            messages=[
                {"role": "system", "content": ORCHESTRATOR_PROMPT},
                {"role": "user", "content": f"Message: {user_message}\nHistory: {history}"}
            ],
            model=self.models["orchestrator"],
            temperature=0.3,
            max_tokens=200
        )
        return json.loads(response["choices"][0]["message"]["content"])

    async def audrey_response(self, user_message: str, history: list, rag_context: str) -> str:
        """Audrey agent call."""
        prompt = AUDREY_SYSTEM_PROMPT.format(
            rag_context=rag_context,
            history=format_history(history)
        )

        response = await self.chat_completion(
            messages=[
                {"role": "system", "content": prompt},
                {"role": "user", "content": user_message}
            ],
            model=self.models["audrey"],
            temperature=0.7,
            max_tokens=1000
        )
        return response["choices"][0]["message"]["content"]

    async def carole_response(self, user_message: str, history: list, rag_context: str) -> str:
        """Carole agent call."""
        prompt = CAROLE_SYSTEM_PROMPT.format(
            rag_context=rag_context,
            history=format_history(history)
        )

        response = await self.chat_completion(
            messages=[
                {"role": "system", "content": prompt},
                {"role": "user", "content": user_message}
            ],
            model=self.models["carole"],
            temperature=0.7,
            max_tokens=1000
        )
        return response["choices"][0]["message"]["content"]
```

---

## âš¡ Optimisations Concurrence

### Main Handler Async

```python
async def handle_message(
    user_message: str,
    conversation_id: str,
    user_id: str
) -> dict:
    """
    Main async handler avec parallÃ©lisation optimale.
    """

    # Step 1: Parallel fetch de history et orchestration
    history_task = asyncio.create_task(
        load_conversation_history(conversation_id, limit=10)
    )

    # Wait history avant orchestration
    history = await history_task

    # Step 2: Orchestrator dÃ©cide
    decision = await openrouter.orchestrate(user_message, history)

    if decision["agent"] == "escalate":
        return {
            "response": "Je vais transfÃ©rer ta demande Ã  l'Ã©quipe humaine qui pourra mieux t'aider! ğŸ¤",
            "agent": "escalate",
            "next_action": "human_handoff"
        }

    # Step 3: Agent-specific RAG + Response (parallel)
    if decision["agent"] == "audrey":
        rag_context = await rag_audrey(user_message)
        response = await openrouter.audrey_response(user_message, history, rag_context)
    else:  # carole
        rag_context = await rag_carole(user_message)
        response = await openrouter.carole_response(user_message, history, rag_context)

    # Step 4: Post-process
    loop_detected = detect_conversation_loop(history + [{"role": "user", "content": user_message}])

    if loop_detected:
        response += "\n\nğŸ’¡ Tu as beaucoup de questions approfondies! Notre formation pourrait t'intÃ©resser pour un accompagnement personnalisÃ©."

    # Step 5: Save messages (async, non-bloquant)
    asyncio.create_task(
        save_messages(conversation_id, user_message, response, decision["agent"])
    )

    return {
        "response": response,
        "agent": decision["agent"],
        "confidence": decision["confidence"],
        "loop_detected": loop_detected
    }
```

### Timing Attendu

```
â”œâ”€ Load history: 150ms
â”œâ”€ Orchestrator: 400ms (Haiku rapide)
â”œâ”€ RAG pipeline: 800ms
â”‚  â”œâ”€ Embedding: 100ms
â”‚  â”œâ”€ Vector search: 200ms
â”‚  â””â”€ Rerank: 500ms
â”œâ”€ Agent response: 2-3s (Sonnet)
â”œâ”€ Post-process: 50ms
â””â”€ Save (async): 200ms (non-bloquant)

TOTAL PERÃ‡U PAR USER: ~3.5-4.5s âœ…
```

---

## ğŸ“Š MÃ©triques & Monitoring

### Logs StructurÃ©s

```python
{
  "timestamp": "2025-11-03T12:00:00Z",
  "conversation_id": "uuid",
  "user_message_length": 150,
  "orchestrator_decision": {
    "agent": "carole",
    "confidence": 0.92,
    "reasoning": "Question sur stratÃ©gie Instagram"
  },
  "rag_results": {
    "chunks_found": 20,
    "reranked_top_3": ["doc1", "doc2", "doc3"],
    "sources": ["Formation Insta Pro", "Guide Reels"]
  },
  "agent_response_length": 280,
  "total_time_ms": 3850,
  "model_used": "anthropic/claude-3.5-sonnet",
  "tokens_used": {
    "prompt": 1200,
    "completion": 350
  },
  "cost_usd": 0.018
}
```

---

## ğŸ¯ Prochaines Ã‰tapes

Je vais crÃ©er:

1. âœ… Architecture documentÃ©e
2. ğŸ“ `backend/agents.py` - Code complet agents
3. ğŸ“ `backend/rag_pipeline.py` - Pipeline RAG dual
4. ğŸ“ `backend/orchestrator.py` - Orchestration logique
5. ğŸ“ `backend/openrouter_client.py` - Client OpenRouter
6. ğŸ“ `backend/main.py` - FastAPI endpoint (alternative n8n)
7. ğŸ“ `n8n/workflow-v2.json` - Workflow n8n optimisÃ©
8. ğŸ“ `deployment/setup.md` - Guide dÃ©ploiement complet

**Tu prÃ©fÃ¨res quoi?**
- A) Tout en Python (FastAPI) â†’ Plus rapide, plus performant
- B) Hybride: n8n orchestre + Python nodes
- C) Pur n8n avec Code nodes

Dis-moi et je code tout! ğŸš€
